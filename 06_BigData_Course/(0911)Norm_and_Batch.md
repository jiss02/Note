## 정규화

모델이 복잡해질 수록 세타의 계수들이 높아지고, 세타들이 많아짐.

오버피팅을 막기위한 테크닉두개가있는데, L1과 L2가 있다. 

L1의 경우, 세타에 절댓값을 씌워 합을 취하는 것이고

> 중요하지 않은 피쳐를 배제하는 것에 도움이 된다.

L2의 경우, 세타에 제곱을 통해 합을 취하는 것이다.

> 큰 값을 가진 가중치를 더욱 제약하는 효과가 있다.



loss를 줄이는 것이 모델의 목표이기 때문에, MSE가 낮아질수록 모델의 정확도는 높아지지만 가중치가 더 복잡해져 R(세타)가 올라간다. 둘은 반비례 관계이며, 모델이 무조건 cost func만 줄이는 방향으로 가는 것이 아닌 과적합을 막는 방향으로 가도록 하기위해 이를 사용한다.



MSE만 보면서 줄이지 말고, R(세타)도 보면서 줄이라는 것이다.

람다가 커지면 커질수록 오버피팅막는 것을 빡세게 먹이는 것이다.

가중치를 찍어 누르는 것이다.

L1 L2 둘다 쓸 수 있는데, 이는 일레스틱넷이다. ElasticNet!



## 배치

Stochastic : 데이터를 하나씩 넣는 것

mini-batch : 묶음으로 넣어주는 것

full-batch : 전부 넣어주는 것



## Batch Normalization

Activation func 전에 Normalization을 하는 것이다.

원래는 인풋 레이어에 들어오는 데이터에만 적용했었다.

1. 배치데이터의 평균
2. 배치데이터의 분산계산
3. Normalization 적용 = { ( 데이터 - 평균 ) / 루트( 표준편차 + 앱실론 ) }
4. Standardization 적용 ==> 선형적이게 된다.



## Standardization

Standardization 후 열마다의 평균값은 0 그리고 표준편차값은 1로 맞춰지며, 데이터는 -1 ~ 1사이의 값을 가지게 된다.

이를 통해 시그모이드 도출값을 0.2 ~ 0.7 사이로 바꾸면서 시그모이드의 단점인 비선형성을 해결한다.



## 드랍아웃

인공신경망을 고의로 끊어 오버피팅을 방지하는 방법이다.

