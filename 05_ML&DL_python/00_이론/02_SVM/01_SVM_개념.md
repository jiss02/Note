# SVM
> support vector machine

분류나 회귀분석에 사용된다.


## linear classifier 관점에서의 SVM

linear combination에 기초하여 인스턴스의 class를 분류한다.

- linear combination은 다음과 같은 형태를 일컫는다.


$$
f(x) = w1x1 + w2x2 + ...
$$

여기서 x는 데이터의 attribute를 의미하며 w는 x의 계수이다.

## SVM은 어떠한 선을 찾는가?

svm은 margin(여백)을 최대화하는 선을 찾는다. 

1. 서포트 벡터를 찾는다
2. 서포트 벡터를 기준으로 선을 그어 가장 넓은 마진을 찾고
3. 그 가운데에 선을 그리는 것

이 svm의 전략이다.

### margin과 서포트 벡터란

 __margin__
 
두개의 클래스로 분류된 인스턴스들이 평면위에 찍혀있다고 가정할 때, 두 그룹사이의 가장 넓은 여백을 찾는다. 이를 마진이라 한다. 

__서포트 벡터__

다른 클래스에 속한 데이터가 등장하면 더 이상 선이 전진하지 못하게 되는데, 이 점을 서포트 벡터라고 한다.

- 마진 결정에 영향을 끼치는 관측치

> 그림 설명

## 클래스가 뒤섞여 있는경우

현실의 데이터는 두개의 클래스가 깔끔하게 나누어져 갈라져있지 않다. 클래스가 서로 뒤섞여 선을 긋기 곤란한 상황이 연출되곤한다. 

이러한 경우 마진에서 벗어난 인스턴스를 잘못분류된 것으로 간주하여 해당 데이터에 패널티를 부과한다.

__패널티__

- 패널티를 적용하는 시작점은 서포트 벡터를 지나면서 부과하기 시작한다.

> 마진을 벗어나는 시점

- 거리가 멀어지면 멀어질 수록 패널티의 값은 커진다

__hinge loss function__

패널티의 크기는 hinge loss function을 통해 알아볼 수 있다. 

- 기울기를 통해 패널티의 강도를 조절할 수 있으며, 패널티의 강도는 데이터를 분석하는 사람이 중시하는 방향으로 설정하면 된다. 

> 그림 설명


## Logistic regression 과의 차이점

logistic regression 이 클래스 구분을 최대한으로 일치시키려는 전략을 사용하려 한다면, svm은 margin과 패널티라는 개념을 사용하여 약간의 에러를 허용하는 전략을 사용한다고 볼 수 있다.

최대한 클래스의 구분을 일치시키려 하는 logistic regression의 경우 traning 데이터에 과적합된 선이 그려질 수 있다는 위험이 존재한다.

> 각자의 차이점 그림 설명
