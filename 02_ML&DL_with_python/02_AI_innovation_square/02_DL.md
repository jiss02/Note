## NN (Neural Network)

평범한 Logistic만으로는 XOR문제를 해결할 수 없다. 직선으로 표현이 불가능 하기 때문에, 트레이닝 데이터와 똑같은 데이터로 예측을 해도 좋지 않은 성능이 나온다.

> 트레이닝을 해보면, error가 특정 값에서 부터 내려가지 않는다.

이를 해결하고자 떠오른 것이 바로 딥러닝이다.

인간의 뇌는 수많은 뉴런들이 연결되어 있으며, 다양한 자극에대한 정보를 조합해 임계치(threshold)를 넘으면 활성화를 시키고 수준에 도달하지 못하면 출력을 전달하지 않는다. 즉, 입력에 비례해 출력을 내는 것이 아니라, 입력값들의 모든 합이 임계점에 도달해야만 출력신호를 발생시키는 것이다.

Logistic regression에서 사용했던 로직과 비슷하지 않은가?

1. linear regression을 통해 각 입력에 대해 가중치를 곱한뒤 바이어스를 더해준값을 (`y = x*W + b`)

   > 다양한 입력을 조합한다.

2. sigmoid라는 함수를 통과시켜 0~1사이의 값으로 만들어 0.5이상은 1, 이하는 0이 되도록만들었다.

   > 이를 뉴런의 신호 관점으로 보면 1이면 신호를 출력하고, 0이면 그렇지 않다는 것과 같다고 볼수 있다.

위와같이 활성화 여부를 판단하기 위해 통과시키는 것이 바로 활성화 함수이며, sigmoid이외에도 relu, tanh 등 다양한 함수들이 존재한다.

활성화 함수에 넣어주는 값은, linear regression에서 도출된 값이다. 한마디로, **딥러닝은 linear regression과 activation function의 조합**이라 볼 수 있는 것이다. 

### Node 란

1개의 Logistic regression을 나타낸다.



## 딥러닝의 기본 구조

Input layer과 Hidden layer, Output layer으로 나뉘며, 각각의 층안에는 노드가 존재한다.

입력데이터의 노드 수는 정답을 만들어내는 입력 데이터의 개수 만큼이고, Output layer의 경우 정답의 개수만큼 노드가 필요하다.

- 입력 층: x의 수 (컬럼의 수)
- 출력 층: 클래스의 개수 (어떤 클래스에 속할 확률이 가장 큰지 보기 위함이다)

### Output layer 노드의 수가 클래스 개수인 이유

Output layer의 각 노드를 벡터로 표시하고, `np.argmax`를 통해 가장 큰값의 인덱스를 return 받는다.

각 노드의 인덱스가 어느 클래스를 나타내는 지만 알면 어느 클래스에 속하는지를 쉽게 찾아낼 수 있는 것이다.

### one-hot encoding

**result = [0.1, 0.001, 0.2, 0.1, 0.0001, 0.85, 0.41, 0.3, 0.001]**

**answer = [0,0,0,0,0,1,0,0,0,0]**

result에서 5번째 인덱스의 값이 가장 큰 것을 알 수 있다. 

정답 벡터를 보자. 5번째 인덱스가 1이고 나머지가 0인 것을 보아, 5번째 인덱스가 정답이라는 것을 알수 있다. 이렇듯 정답에 해당하는 인덱스의 원소는 1로 나머지는 0으로 나타내는 표기법을 원-핫 인코딩 이라고 한다.

`정답 변수에는 클래스의 개수가 크기인 벡터의 형태로 들어있다. 답인 것은 1로 아닌 것은 전부 0으로 채운다.`

> result와 answer의 오차를 계산하고 미분을 통해 뒤로 가며 값을 조정함으로써 오차를 줄여나갈 수 있다.

### Hidden layer

Hidden layer의 노드가 많을 수록 좀 더 결과가 정확해지지만, 노드가 많아질수록 시간이 오래걸리므로 적절히 선택해야 한다. Hidden layer에 속한 노드의 수는 정해진 수가 없으며 하나의 하이퍼 파라미터 이다.

Hidden layer 자체의 개수도 하이퍼 파라미터이다. Hidden layer은 한개여도 되고, 여러 개여도 된다. Hidden layer 자체 또한 여러개 일 수록 정으답이 정확해진다. 

Hidden layer의 개수를 많이 만들고, Hidden layer에 속한 노드가 많을 수록 점점 더 깊게 들어가는 모양새가 되어 딥러닝이라고 부르는 것이다.



## 딥러닝 실습 완료 및 정리

__히든노드의 목적__ 

데이터에게 다양성을 주는 것이다. 데이터가 다양한 노드로 갈라져 갈 수 있기 때문이다.

hidden layer가 늘어나게 되면 너무 느려질 수도 있으니 노드의 수를 줄여주자.

> 너무 많이 노드를 주게되면 데이터가 가는 길이 너무 다양해지므로 적당히 주자.

1. input / output / hidden 노드 수 정하기

2. W와 b의 shape 정하기

3. feed_forward, loss, predict, train, 

4. 클래스를 만들어 각 기능을 묶어주고 생성자를 정의하자.

5. 데이터를 만들어내는 Learning, DataGeneration, Verification 클래스 정도는 따로 만드는 것이 좋다. 서로 객체들끼리 왔다갔다 하는 것이다!

   > 사용자들을 위해 UI 클래스를 만들어 주고 싶다면.. UI 클래스를 만들어 사용자로 부터 UI를 통해 입력 받고 이를 data_generate class 로 보내주자.

   

## MNIST

## 분석 전에

행렬을 input으로 넣어주기 전에 1차원으로 늘려준뒤 넣어주자. 그리고, 이미지를 출력할 때는 무조건 행렬로 출력해주자.

> 첫 열이 정답 데이터이다. 



## one-hot 인코딩 (원핫인코딩)

시그모이드는 보통 확률을 나타내기 위해 사용되기 때문에, MNIST는 0과 1의 classfication과는 다르다. 이를 해결하기위해 출력노드를 정답의 개수만큼 생성한다. MNIST의 경우 0~9이므로 10개를 생성하는 것이다. 0 ~ 9 에 해당하는 각 노드는 확률(시그모이드값)을 가지며, 그 확률이 제일 높은 노드(가장 큰 값을 내는 시그모이드)를 선택하여 정답으로 내보낸다. 

가장 큰 값을 내보내는 노드의 인덱스를 우리가 알 수 만 있다면, 정답을 알 수 있는 것이다! 이 경우 정답을 one-hot 인코딩 할 수 있는데, 정답인 인덱스를 1로 두고, 정답이 아닌 인덱스는 0으로 두는 것이다.

**출력값들 중 가장 큰값의 노드 인덱스를 뽑아내는 것이 one-hot 인코딩인 것이다.**

### np.argmax

최대값의 인덱스를 리턴해주는 함수이다. 원핫인코딩시 아주 유용하다!

### 주제 이외의 수업

__Standardization?__

최대값으로 정규화를 하는 것은 기본적인 아이디어라고 할 수 있으나, 단점이 존재한다. max 값이 0인 경우이다. 이런 경우를 대비해 다른 방법이 여러가지 존재하는데, 이는 추후에 배우도록 한다.

__약간의 테크닉?__

임의의 데이터는 언제나 최대값과 최소값 사이에 있을 것이다. 최대값이 0 인 경우 최대값에 대하여 나눌 수 없으니, 양번을 최소값으로 빼는 테크닉을 쓸 수 있다.

```
min - min <= x <= max - min
```

이제 3변을 max - min 으로 나누자.

```
0 <= (x - min) / (max - min) <= 1
```

이된다. max - min이 늘 양수이기 때문에 사용하기 안전하다. min과 max가 같아져 0이 되는 경우가 있을 수 있으나, 이런 경우 정규화가 필요하지 않은 경우이기 때문에 굳이 정규화를 사용하지 않아도 되는 것이다!

__Preprocessing__

1. 데이터를 분석하기 위해서는 숫자로 바꾸어 주어야한다.
2. 데이터 분석을 실행할 때 큰 영향을 미치지 않는 데이터를 골라 열을 줄이자.
3. Null 값이나 이상치를 어떻게 처리할지 정하자.
4. 정규화를 어떻게 할지 고민하자.

### cnn

cnn은 데이터의 특징을 뽑아내는 것이다. 이후는 실습으로 진행한다.